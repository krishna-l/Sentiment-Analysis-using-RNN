{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis from movie reviews using RNN.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWnVuxQ245YO",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis from movie reviews using RNN\n",
        "We will be using IMDb dataset which consists of user movie reviews of whether the user liked the movie or not.\n",
        "We are going to understand the language written and keep them in memory over time and predict the sentiment of the user in each review.\n",
        "Here we are using LSTM cells because we dont want to forget the words too quickly. So that words early in a sentence will affect meaning of that sentence significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Vuc9Gw71d9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd3se2wE8QuW",
        "colab_type": "text"
      },
      "source": [
        "Lets load our train and test data. Lets limit the popular words to 50000 words loaded into our training and test data. for some reason we have 25000 training reviews and 25000 testing reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoKHdQYx8ocA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "21c9c568-bda8-4a3b-e966-b6757b0f2174"
      },
      "source": [
        "(x_train, y_train),(x_test,y_test) = imdb.load_data(num_words=50000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSAArBBs9B-c",
        "colab_type": "text"
      },
      "source": [
        "Lets look at how the data is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9FyNkNP9FL6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "936b6c83-b990-423a-a49c-5677eb2cae0d"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoEoNyu29Kw2",
        "colab_type": "text"
      },
      "source": [
        "Well thats just numbers :P. Well keras have already convert the each unique word to numbers and we have limited those numbers to 50000 so that we have only those popular words to work with to determine the sentiment.\n",
        "We cannot read these reviews but it saves a lot of time during pre processing.\n",
        "Lets look at the corresponding y_train. It will have 0 or 1 which corresponds to the dislike and like of that movie "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBARbqQM982W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "440dc7dd-2edd-4446-edf0-25ef7e6a1a66"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1fXc-1f-DJH",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to limit the review to 150 words per review as RNNs are very resource intensive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YdyuaYD-Vdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We are going to trim the reviews to first 150 words.\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=150)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Wc1U0t-xv-",
        "colab_type": "text"
      },
      "source": [
        "Lets build the model and take a look how it is programmed. \n",
        "We will start with a Embedding layer, this is a step that converts the input data into dense vectors of fixed size that's better suited for a NN. So our 150 word review which has only 50000 frequent words will be converted/ funneled into a dense vector of 128 neurons.\n",
        "Then we will set up our RNN with a LSTM of 128 recurrent neuron and a dropout of 20% to prevent overfitting.\n",
        "Finally we will have a single output neuron which choses out our binary sentiment classification of 0 or 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tq6bMVZ_6GX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "c0111545-f342-4a73-ca5f-b8b88d01272f"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(50000, 128)) #Reducing our words to vectors of 128 neurons\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))#LSTM with 128 recurrent neuron and a dropout of 20%\n",
        "model.add(Dense(1, activation='sigmoid'))#OP layer\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 128)         6400000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,531,713\n",
            "Trainable params: 6,531,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5BApIGPCbbC",
        "colab_type": "text"
      },
      "source": [
        "Since its a binary classification we will use binary_crossentropy loss function and adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnS5VYO5CpP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Osul5H3C9FO",
        "colab_type": "text"
      },
      "source": [
        "Lets train the model. This will take a lot of time.\n",
        "We will have a batch size of 32 and 15 epochs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6ranhT9F4t4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "871247b2-c50c-4f41-ad70-7d63417c27d6"
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=2, validation_data=(x_test,y_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 - 258s - loss: 0.4631 - acc: 0.7823 - val_loss: 0.4802 - val_acc: 0.7729\n",
            "Epoch 2/15\n",
            "25000/25000 - 255s - loss: 0.2853 - acc: 0.8877 - val_loss: 0.3894 - val_acc: 0.8394\n",
            "Epoch 3/15\n",
            "25000/25000 - 255s - loss: 0.1749 - acc: 0.9366 - val_loss: 0.4098 - val_acc: 0.8494\n",
            "Epoch 4/15\n",
            "25000/25000 - 254s - loss: 0.1199 - acc: 0.9573 - val_loss: 0.4580 - val_acc: 0.8314\n",
            "Epoch 5/15\n",
            "25000/25000 - 251s - loss: 0.0785 - acc: 0.9728 - val_loss: 0.5004 - val_acc: 0.8413\n",
            "Epoch 6/15\n",
            "25000/25000 - 252s - loss: 0.0491 - acc: 0.9837 - val_loss: 0.5894 - val_acc: 0.8394\n",
            "Epoch 7/15\n",
            "25000/25000 - 258s - loss: 0.0356 - acc: 0.9881 - val_loss: 0.6645 - val_acc: 0.8417\n",
            "Epoch 8/15\n",
            "25000/25000 - 254s - loss: 0.0257 - acc: 0.9920 - val_loss: 0.7935 - val_acc: 0.8312\n",
            "Epoch 9/15\n",
            "25000/25000 - 256s - loss: 0.0210 - acc: 0.9930 - val_loss: 0.7761 - val_acc: 0.8356\n",
            "Epoch 10/15\n",
            "25000/25000 - 250s - loss: 0.0154 - acc: 0.9949 - val_loss: 0.8851 - val_acc: 0.8422\n",
            "Epoch 11/15\n",
            "25000/25000 - 252s - loss: 0.0130 - acc: 0.9954 - val_loss: 0.8841 - val_acc: 0.8332\n",
            "Epoch 12/15\n",
            "25000/25000 - 252s - loss: 0.0125 - acc: 0.9960 - val_loss: 0.9271 - val_acc: 0.8267\n",
            "Epoch 13/15\n",
            "25000/25000 - 251s - loss: 0.0076 - acc: 0.9979 - val_loss: 0.8632 - val_acc: 0.8332\n",
            "Epoch 14/15\n",
            "25000/25000 - 251s - loss: 0.0038 - acc: 0.9991 - val_loss: 1.0863 - val_acc: 0.8366\n",
            "Epoch 15/15\n",
            "25000/25000 - 250s - loss: 0.0087 - acc: 0.9969 - val_loss: 1.0146 - val_acc: 0.8298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fed4f798438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNduhbsNVSnh",
        "colab_type": "text"
      },
      "source": [
        "Lets evaluate our test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TePMuKgVaZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c775ab4e-7e26-4ec2-fe86-da6fdf13e0b6"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test, batch_size=32, verbose=2)\n",
        "print('Test Score: ',score)\n",
        "print('Test Accuracy: ', acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 - 28s - loss: 1.0146 - acc: 0.8298\n",
            "Test Score:  1.014570419074893\n",
            "Test Accuracy:  0.82976\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}